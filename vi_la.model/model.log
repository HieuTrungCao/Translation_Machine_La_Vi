[Fri, 10 Nov 2023 08:58:11 INFO] .vi * src vocab size = 11747
[Fri, 10 Nov 2023 08:58:11 INFO] .lo * tgt vocab size = 12548
[Fri, 10 Nov 2023 08:58:11 INFO] Building model...
[Fri, 10 Nov 2023 08:58:12 INFO] Transformer(
  (encoder): Encoder(
    (embed): Embedding(11747, 512)
    (pe): PositionalEncoder(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0-5): 6 x EncoderLayer(
        (norm_1): Norm()
        (norm_2): Norm()
        (attn): MultiHeadAttention(
          (q_linear): Linear(in_features=512, out_features=512, bias=True)
          (k_linear): Linear(in_features=512, out_features=512, bias=True)
          (v_linear): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (out): Linear(in_features=512, out_features=512, bias=True)
        )
        (ff): FeedForward(
          (linear_1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_2): Linear(in_features=2048, out_features=512, bias=True)
        )
        (dropout_1): Dropout(p=0.1, inplace=False)
        (dropout_2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): Norm()
  )
  (decoder): Decoder(
    (embed): Embedding(12548, 512)
    (pe): PositionalEncoder(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0-5): 6 x DecoderLayer(
        (norm_1): Norm()
        (norm_2): Norm()
        (norm_3): Norm()
        (dropout_1): Dropout(p=0.1, inplace=False)
        (dropout_2): Dropout(p=0.1, inplace=False)
        (dropout_3): Dropout(p=0.1, inplace=False)
        (attn_1): MultiHeadAttention(
          (q_linear): Linear(in_features=512, out_features=512, bias=True)
          (k_linear): Linear(in_features=512, out_features=512, bias=True)
          (v_linear): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (out): Linear(in_features=512, out_features=512, bias=True)
        )
        (attn_2): MultiHeadAttention(
          (q_linear): Linear(in_features=512, out_features=512, bias=True)
          (k_linear): Linear(in_features=512, out_features=512, bias=True)
          (v_linear): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (out): Linear(in_features=512, out_features=512, bias=True)
        )
        (ff): FeedForward(
          (linear_1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm): Norm()
  )
  (out): Linear(in_features=512, out_features=12548, bias=True)
)
[Fri, 10 Nov 2023 08:58:12 INFO] Encoder: 24929792
[Fri, 10 Nov 2023 08:58:12 INFO] Decoder: 31649792
[Fri, 10 Nov 2023 08:58:12 INFO] * Number of parameters: 56579584
[Fri, 10 Nov 2023 08:58:12 INFO] Starting training on cuda
[Fri, 10 Nov 2023 09:02:05 INFO] epoch: 000 - iter: 00200 - train loss: 2.2267 - time elapsed/per batch: 233.2164 1.1661
[Fri, 10 Nov 2023 09:06:03 INFO] epoch: 000 - iter: 00400 - train loss: 1.8783 - time elapsed/per batch: 237.6505 1.1883
[Fri, 10 Nov 2023 09:10:10 INFO] epoch: 000 - iter: 00600 - train loss: 1.7442 - time elapsed/per batch: 246.5676 1.2328
[Fri, 10 Nov 2023 09:14:39 INFO] epoch: 000 - iter: 00800 - train loss: 1.6735 - time elapsed/per batch: 269.5468 1.3477
[Fri, 10 Nov 2023 09:19:10 INFO] epoch: 000 - iter: 01000 - train loss: 1.6316 - time elapsed/per batch: 271.1215 1.3556
[Fri, 10 Nov 2023 09:23:59 INFO] epoch: 000 - iter: 01200 - train loss: 1.6458 - time elapsed/per batch: 289.1121 1.4456
